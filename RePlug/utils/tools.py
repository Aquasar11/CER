import numpy as np


def ensemble_aggregate(probs, scores):
    weights = scores.squeeze(0)
    # Broadcasting weights to match each tensor in probs and summing them
    weighted_sum = np.sum([weight * prob for weight, prob in zip(weights, probs)], axis=0)

    return weighted_sum


def my_aggregate(probs, scores):
    pass


def aggregate_token_probs(probs, scores, mode):
    if mode == 'ensemble':
        return ensemble_aggregate(probs, scores)
    elif mode == 'new':
        return my_aggregate(probs, scores)

    raise NotImplementedError()


def calculate_perplexity(log_probs):
    """
    Calculate perplexity given a list of log probabilities.
    """
    return np.exp(-np.mean(log_probs))


def evaluate_retrieval(retrieved_docs, ground_truth):
    """
    Simple evaluation of the retriever based on overlap with ground truth.

    Args:
        retrieved_docs (list): List of retrieved document IDs.
        ground_truth (list): List of ground truth document IDs.

    Returns:
        float: Precision metric for retrieved documents.
    """
    retrieved_set = set(retrieved_docs)
    ground_truth_set = set(ground_truth)
    if len(retrieved_set) == 0:
        return 0.0
    return len(retrieved_set & ground_truth_set) / len(retrieved_set)


def evaluate_language_model(predictions, targets):
    """
    Evaluate LM output against a list of ground-truth sequences using accuracy.

    Args:
        predictions (list of str): List of predictions generated by the language model.
        targets (list of str): List of target ground-truth sequences.

    Returns:
        float: Average accuracy of the model.
    """
    correct = sum([1 for pred, target in zip(predictions, targets) if pred.strip() == target.strip()])
    return correct / len(targets) if targets else 0.0
